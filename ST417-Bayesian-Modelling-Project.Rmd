---
title: "ST417 Bayesian Modelling Project"
author: "Fionn McGlacken 19388186, Alex Horkan 19461736, Hugo Mead 19419556"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("stringr")
library("dplyr")
library("lubridate")
library("knitr")
library("LearnBayes")
library("ggplot2")
library("rjags")
library("truncnorm")
source("TeachBayes.r")
```

# Introduction
This project was completed for ST417 Introduction to Bayesian Modelling. It was developed on github

https://github.com/Fionn-McGlacken/ST417-Bayesian-Modelling-Project

# Aim

## Research question: 
What are the distributions of time, distance, and cost of travel for University of Galway students, and how do they vary with respect to confounding variables such as weather, day, time, traffic, age, city or town, and mode of transport?

## Motivation: 
We are in an accommodation crisis. Third-level students without access to accommodation near their universities commute longer hours, over greater distances, at higher costs. We want to understand this empirically.

## Background Information
The target population of our study is University of Galway students. The data used in our study was surveyed from University of Galway students. We collected data on time, distance, cost, weather, day, time, traffic, age, location and mode of transport.

## List of Aims
In order to explore the effects of the different variables associated with commuting, we have set a list of aims we wish to explore throughout the project.

1.	Investigate the relationship between time, distance, and commuting costs.
2.	Investigate the effects of confounding variables, such as weather, day, time, traffic, age, and mode of transport. 
3.	Calculate point and interval estimates of time, distance, and cost to travel. 
4.	Investigate if the average time, distance, and cost of travel for students have increased over previous years. 
5.	Investigate the most expensive city or town to travel from, on average. 
6.	Investigate prevailing weather, the most common mode of transport, and traffic patterns. 
7.	Estimate the carbon footprint of travel for University of Galway students. 
8.	Compare differences between year groups and see which year has the greatest travel times.

# Data and Prior Description

## Subjective Opinion
In our opinion, the time, distance and commuting costs should be the easiest to analyse as they are the most straightforward variables to measure. We believe that the confounding variables will be more difficult to analyse as they are harder to measure. For example, weather and traffic data must be sourced online.

## Data Sampling
### Survey
The data we used for the purposes of this analysis was collected via an online questionnaire which was distributed amongst University of Galway students.

https://docs.google.com/forms/d/1aJ-uViRoRLn2aS7wT3EdJs-szgU1ARKo895fsDlbGL4/edit

We asked students what they think the average time, cost, distance, mode of transport for students is to inform our prior distributions. Then, we asked students about the time, cost, distance and mode of transport in their travel to university to inform the likelihood distributions. We also asked students about their yeargroup and origin. 

We received a total of 74 responses comprised mostly of 3rd and 4th-year students, with the remaining being made up of 1st and 2nd-year as well as postgraduate students.

### Additional Data
We collected additional weather data online. Unfortunately, we could not find reliable traffic data.

## Limitations: Issues in Data Sampling
There were some issues collecting data:
### Biased Data
Firstly, it was easiest to collect data from students we know. This means that our data is not representative of the entire student population. For example, 3rd and 4th year students are overrepresented in our study. Additionally, it may be the case that students who are most affected by the commuting crisis are not present in this study as they were not accessible. Therefore, results may be more positive than in reality. 
### Messy Data
Secondly, we did not impose any constraints on how students can enter the data. This lead to a lot of messy data that needed to be cleaned. For example, one student wrote 'half an hour to 40 minutes' for commute time. To analyse this, the value must be converted to the mean of the range. 
### Missing Traffic Data
We could not find reliable traffic data online.

```{r}
data <- read.csv("data.csv")

# est short for estimated
# naming columns
new_names <- c(
  "timestamp",
  "est_time",
  "est_distance",
  "est_cost",
  "est_transport",
  "year_group",
  "time",
  "start_time",
  "distance",
  "cost",
  "transport",
  "origin")

colnames(data) <- new_names
```

## Cleaning Data
### Functions for Cleaning Data
Defining functions to clean data.

```{r}
# removes letters from entries
strip_data <- function(column) {
  matcher <- "[^0-9.-]"
  cond <- grepl(matcher, column)
  stripped_rows <- str_remove_all(column[cond], matcher)
  column[cond] <- stripped_rows
  return(column)
}

# removes letters and scales entries by scalar e.g. '80 cents' to '0.8' (euro)
scale_data <- function(column, to_match, scalar) {
  matcher <- paste(to_match, collapse = "|")
  cond <- grepl(matcher, column)
  stripped_rows <- str_remove_all(column[cond], "[^0-9.-]")
  stripped_rows <- as.numeric(stripped_rows)
  column[cond] <- stripped_rows * scalar
  return(column)
}

# replaces range entries by average of range e.g. '7-10' to '8.5'
avg_range <- function(column) {
  matcher <- "^(\\d+)-(\\d+)$"
  cond <- grepl(matcher, column)
  column[cond] <- rowMeans(read.table(text = column[cond],
     sep = "-", header = FALSE), na.rm = TRUE)
  return(column)
}

# replaces range entries by scaled average of range
# e.g. '2-4hrs' to '180' (minutes)
scale_and_avg_range <- function(column, to_match, scalar) {
  matcher <- paste(to_match, collapse = "|")
  cond <- grepl(matcher, column)
  stripped_rows <- str_remove_all(column[cond], to_match)
  table <- read.table(text = stripped_rows, sep = "-", header = FALSE) * scalar
  column[cond] <- rowMeans(table, na.rm = TRUE)
  return(column)
}
```

### Cleaning Data

```{r}
data <- data %>% mutate(timestamp = as.Date(timestamp))
data <- data %>% mutate(day = weekdays(timestamp))

data$est_time <- sub("to", "-", data$est_time)
data$est_time <- sub("half an hour", "30", data$est_time)
hours <- c("hours", "hrs")
data$est_time <- scale_and_avg_range(data$est_time, hours, 60)
data$est_time <- strip_data(data$est_time)
data$est_time <- avg_range(data$est_time)
data <- data %>% mutate(est_time = as.numeric(est_time))

data$est_distance <- sub("or", "-", data$est_distance)
data$est_distance <- strip_data(data$est_distance)
data$est_distance <- avg_range(data$est_distance)
data <- data %>% mutate(est_distance = as.numeric(est_distance))

data$est_cost <- sub("/", "-", data$est_cost)
cents <- c("cents")
data$est_cost <- scale_data(data$est_cost, cents, 0.01)
data$est_cost <- strip_data(data$est_cost)
data$est_cost <- avg_range(data$est_cost)
data <- data %>% mutate(est_cost = as.numeric(est_cost))

data$time <- strip_data(data$time)
data <- data %>% mutate(time = as.numeric(time))
data$start_time <- as.POSIXct(paste(data$timestamp, data$start_time)
                                    , format = "%Y-%m-%d %H:%M")

# value entered in wrong column
data[52, 9] <- data[52, 10]
data$distance <- sub("Less than a kilometre", "1", data$distance)
data$distance <- strip_data(data$distance)
data <- data %>% mutate(distance = as.numeric(distance))

data[52, 10] <- NA
data[63, 10] <- "0"
zeros <- c("Nothing", "None")
for (i in zeros) {
  data$cost <- sub(i, "0", data$cost)
}
data$cost <- gsub("Less than a kilometre", paste(zeros, collapse = "|")
                  , data$cost)
costs <- c("cent")
data$cost <- scale_data(data$cost, costs, 0.01)
data[3, 10] <- 0.8
data <- data %>% mutate(cost = as.numeric(cost))

# cleaned data
write.csv(data, "cleaned_data.csv")
```

Next, excel was used to clean location data and add weather data.

```{r}
data <- read.csv("origin_weather_data.csv")
head(data)
```

## Data Analysis
Our data can be divided into three sections
- Estimated data (to inform priors)
- Actual data (to inform likelihoods)
- Participant data (to measure confounds)

We have four estimates (est) to inform our priors
- est_time
- est_distance
- est_cost
- est_transport

We have four types of actual data to inform our likelihoods
- time
- distance
- cost
- transport

We have 10 types of participant data to measure confounds
- timestamp
- year_group
- start_time
- origin
- day
- temp_celcius
- precipitation_mm
- perc_humidity
- wind_kmh
- weather (light, moderate, heavy)

### Time

#### est_time
est_time is the estimated average commute time.

```{r est_time plots}
est_time <- na.omit(data$est_time)
summary(est_time)

par(mfrow=c(1,2))
plot(density(est_time), main="Estimated Commute Time", xlab="Time (minutes)")
boxplot(est_time, main="Estimated Commute Time", xlab="Time (minutes)")
```

#### Time prior
We will use the est_time data to inform our prior for the average commute time.

From est_time, it appears the data are approximately normally distributed. Therefore, a trucated normal prior will be used, with $mu = $ mean of est_time, $\sigma =$ standard deviation of est_time, and a limit at 0 (cannot have negative time).

```{r time prior}
mu <- mean(est_time)
sigma <- sd(est_time)

prior_time <- rtruncnorm(71, a=0, b=Inf, mu, sigma)
par(mfrow=c(1,2))
plot(density(prior_time), main = 'Prior for average time', xlab="Time (minutes)")
boxplot(prior_time, main="Estimated Commute Time", xlab="Time (minutes)")

# bayes_df_time <- data.frame(theta_time, prior_time)
# bayes_df_time
# prob_plot(bayes_df_time)
```

### Distance

```{r time posterior}
xbar_time <- mean(data$time, na.rm = TRUE)
n_time <- NROW(data$time)
sigma_time <- sd(data$time, na.rm = TRUE)
se_time <- sigma_time / sqrt(n_time)

# likelihood calculation
likelihood_time <- dnorm(xbar_time, mean = est_time, sd = se_time)
bayes_df_time <- data.frame(est_time, prior_time, likelihood_time)

# posterior calculation
bayes_df_time$Product <- bayes_df_time$prior_time * bayes_df_time$likelihood_time # nolint
bayes_df_time$Posterior <- bayes_df_time$Product / sum(bayes_df_time$Product)
bayes_df_time

# prior and posterior comparison
prior_post_plot(bayes_df_time)
```

```{r distance prior}
theta_dist <- na.omit(data$est_distance)
prior_dist <- dnorm(theta_dist, mean = mean(theta_dist), sd = sd(theta_dist))
bayes_df_dist <- data.frame(theta_dist, prior_dist)
bayes_df_dist
prob_plot(bayes_df_dist)
```

```{r distance posterior}
xbar_dist <- mean(data$distance, na.rm = TRUE)
n_dist <- NROW(data$distance)
sigma_dist <- sd(data$distance, na.rm = TRUE)
se_dist <- sigma_dist / sqrt(n_dist)

# likelihood calculation
likelihood_dist <- dnorm(xbar_dist, mean = theta_dist, sd = se_dist)
bayes_df_dist <- data.frame(theta_dist, prior_dist, likelihood_dist)

# posterior calculation
bayes_df_dist$Product <- bayes_df_dist$prior_dist * bayes_df_dist$likelihood_dist # nolint
bayes_df_dist$Posterior <- bayes_df_dist$Product / sum(bayes_df_dist$Product)
bayes_df_dist

# prior and posterior comparison
prior_post_plot(bayes_df_dist)
```

```{r cost prior}
theta_cost <- na.omit(data$est_cost)
prior_cost <- dnorm(theta_cost, mean = mean(theta_cost), sd = sd(theta_cost))
bayes_df_cost <- data.frame(theta_cost, prior_cost)
bayes_df_cost
prob_plot(bayes_df_cost)
```

```{r cost posterior}
xbar_cost <- mean(data$cost, na.rm = TRUE)
n_cost <- NROW(data$cost)
sigma_cost <- sd(data$cost, na.rm = TRUE)
se_cost <- sigma_cost / sqrt(n_cost)

# likelihood calculation
likelihood_cost <- dnorm(xbar_cost, mean = theta_cost, sd = se_cost)
bayes_df_cost <- data.frame(theta_cost, prior_cost, likelihood_cost)

# posterior calculation
bayes_df_cost$Product <- bayes_df_cost$prior_cost * bayes_df_cost$likelihood_cost # nolint
bayes_df_cost$Posterior <- bayes_df_cost$Product / sum(bayes_df_cost$Product)
bayes_df_cost

# prior and posterior comparison
prior_post_plot(bayes_df_cost)
```

```{r}
m0_time <- mean(data$est_time, na.rm = TRUE)
m0_dist <- mean(data$est_distance, na.rm = TRUE)
m0_cost <- mean(data$est_cost, na.rm = TRUE)
s0_time <- sd(data$est_time, na.rm = TRUE)
s0_dist <- sd(data$est_distance, na.rm = TRUE)
s0_cost <- sd(data$est_cost, na.rm = TRUE)
```

```{r}
post_mean_time <- ((1 / (se_time^2) + 1 / (s0_time^2))^(-1)) * (xbar_time / (se_time^2) + m0_time / (s0_time^2)) # nolint
post_sd_time <- sqrt((1 / (se_time^2) + 1 / (s0_time^2))^(-1))

post_mean_dist <- (1 / (se_dist^2) + 1 / (s0_dist^2))^(-1) * (xbar_dist / (se_dist^2) + m0_dist / (s0_dist^2)) # nolint
post_sd_dist <- sqrt((1 / (se_dist^2) + 1 / (s0_dist^2))^(-1))

post_mean_cost <- (1 / (se_cost^2) + 1 / (s0_cost^2))^(-1) * (xbar_cost / (se_cost^2) + m0_cost / (s0_cost^2)) # nolint
post_sd_cost <- sqrt((1 / (se_cost^2) + 1 / (s0_cost^2))^(-1))
```

```{r 95 credible intervals}
# 95% credible intervals
time_sims <- rnorm(10000, mean = post_mean_time, sd = post_sd_time)
time_point_estimate <- mean(time_sims)
time_point_estimate

time_ci95 <- quantile(time_sims, probs = c(0.025, 0.975))
time_ci95
normal_interval(0.95, time_ci95)

dist_sims <- rnorm(10000, mean = post_mean_dist, sd = post_sd_dist)
dist_point_estimate <- mean(dist_sims)
dist_point_estimate

dist_ci95 <- quantile(dist_sims, probs = c(0.025, 0.975))
dist_ci95
normal_interval(0.95, dist_ci95)

cost_sims <- rnorm(10000, mean = post_mean_cost, sd = post_sd_cost)
cost_point_estimate <- mean(cost_sims)
cost_point_estimate

cost_ci95 <- quantile(cost_sims, probs = c(0.025, 0.975))
cost_ci95
normal_interval(0.95, cost_ci95)
```

```{r 95 credible intervals plot (test)}
# # 95% credible intervals plot
# x <- seq(from = 0, to = 100, length.out = 1000)

# priorx_time <- dnorm(x, mean = mean(data$est_time, na.rm = TRUE), sd = sd(data$est_time, na.rm = TRUE)) # nolint
# priorx_dist <- dnorm(x, mean = mean(data$est_distance, na.rm = TRUE), sd = sd(data$est_distance, na.rm = TRUE)) # nolint
# priorx_cost <- dnorm(x, mean = mean(data$est_cost, na.rm = TRUE), sd = sd(data$est_cost, na.rm = TRUE)) # nolint

# datax_time  <- dnorm(x, mean = mean(data$time, na.rm = TRUE), sd = se_time)
# datax_dist  <- dnorm(x, mean = mean(data$distance, na.rm = TRUE), sd = se_dist)
# datax_cost  <- dnorm(x, mean = mean(data$cost, na.rm = TRUE), sd = se_cost)

# postx_time  <- dnorm(x, mean = post_mean_time, sd = post_sd_time)
# postx_dist  <- dnorm(x, mean = post_mean_dist, sd = post_sd_dist)
# postx_cost  <- dnorm(x, mean = post_mean_cost, sd = post_sd_cost)

# plot(x, priorx_time, type = "l", col = "blue", lwd = 2, xlab = "Time (min)", ylab = "Density", main = "Prior and Posterior Distributions") # nolint
# lines(x, datax_time, col = "red", lwd = 2)
# lines(x, postx_time, col = "green", lwd = 2)

# legend("topright", legend = c("Prior", "Data", "Posterior"), col = c("blue", "red", "green"), lwd = 2) # nolint
```


```{r distributions plot}
x <- seq(-60, 80, length = 150)

priorx_time <- dnorm(x, mean = mean(data$est_time, na.rm = TRUE),
                     sd = sd(data$est_time, na.rm = TRUE))
priorx_dist <- dnorm(x, mean = mean(data$est_distance, na.rm = TRUE),
                     sd = sd(data$est_distance, na.rm = TRUE))
priorx_cost <- dnorm(x, mean = mean(data$est_cost, na.rm = TRUE),
                     sd = sd(data$est_cost, na.rm = TRUE))

datax_time  <- dnorm(x, mean = mean(data$time, na.rm = TRUE), sd = se_time)
datax_dist  <- dnorm(x, mean = mean(data$distance, na.rm = TRUE), sd = se_dist)
datax_cost  <- dnorm(x, mean = mean(data$cost, na.rm = TRUE), sd = se_cost)

postx_time <- dnorm(x, mean = post_mean_time, sd = post_sd_time)
postx_dist <- dnorm(x, mean = post_mean_dist, sd = post_sd_dist)
postx_cost <- dnorm(x, mean = post_mean_cost, sd = post_sd_cost)

plot(x, priorx_time, type = "l", lwd = 3, xlim = c(-40, 60), ylim = c(0, 0.8),
     col = "red4", main = "", xlab = "theta", ylab = "")
lines(x, priorx_dist, col = "green4", lwd = 3)
lines(x, priorx_cost, col = "orange3", lwd = 3)

lines(x, datax_time, col = "black", lwd = 3)
lines(x, datax_dist, col = "black", lwd = 3)
lines(x, datax_cost, col = "black", lwd = 3)

lines(x, postx_time, col = "red", lwd = 3)
lines(x, postx_dist, col = "green", lwd = 3)
lines(x, postx_cost, col = "orange", lwd = 3)

legend("topright",
       c("Time Prior", "Distance Prior", "Cost Prior", "Data",
        "Time Post", "Distance Post", "Cost Post"),
       lty = 1, lwd = 3,
       col = c("red4", "green4", "orange3", "black", "red", "green", "orange")
       )
```


## Bayesian Analysis with rjags

```{r rjags, include=FALSE}
library(rjags)
```

```{r define model}
# define model
model <- "
model {
  for (i in 1:N) {
    time[i] ~ dnorm(mu_time, sigma_time)
    distance[i] ~ dnorm(mu_dist, sigma_dist)
    cost[i] ~ dnorm(mu_cost, sigma_cost)
  }
  mu_time ~ dnorm(m0_time, sigma0_time)
  mu_dist ~ dnorm(m0_dist, sigma0_dist)
  mu_cost ~ dnorm(m0_cost, sigma0_cost)
  sigma_time ~ dgamma(a0_time, b0_time)
  sigma_dist ~ dgamma(a0_dist, b0_dist)
  sigma_cost ~ dgamma(a0_cost, b0_cost)
}
"
```

```{r model data}
# data
data <- list(
  N = nrow(data),
  time = data$time,
  distance = data$distance,
  cost = data$cost,
  m0_time = m0_time,
  m0_dist = m0_dist,
  m0_cost = m0_cost,
  sigma0_time = 1 / (s0_time^2),
  sigma0_dist = 1 / (s0_dist^2),
  sigma0_cost = 1 / (s0_cost^2),
  a0_time = 1 / (se_time^2),
  a0_dist = 1 / (se_dist^2),
  a0_cost = 1 / (se_cost^2),
  b0_time = 1 / (se_time^2) * xbar_time,
  b0_dist = 1 / (se_dist^2) * xbar_dist,
  b0_cost = 1 / (se_cost^2) * xbar_cost
)
```

```{r compile model}
# compile model
jags_model <- jags.model(
  textConnection(model),
  data = data,
  inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)
)
```

```{r burn in model}
# burn in model
update(jags_model, 1000)
```

```{r simulate model}
# simulate posterior
sims <- coda.samples(
  jags_model,
  variable.names = c("mu_time", "mu_dist", "mu_cost",
                     "sigma_time", "sigma_dist", "sigma_cost"),
  n.iter = 10000
)
```

```{r sims plot}
# plot sims
plot(sims)
```

```{r sims summary}
# summary
summary(sims)
```
```{r compile mcmc}
# compile model
jags_model_mc <- jags.model(
  textConnection(model),
  data = data,
  n.chains = 3
)
```
```{r burn in mcmc}
# burn in model
update(jags_model_mc, 1000)
```
```{r simulate mcmc}
# simulate posterior
sims_mc <- coda.samples(
  jags_model_mc,
  variable.names = c("mu_time", "mu_dist", "mu_cost",
                     "sigma_time", "sigma_dist", "sigma_cost"),
  n.iter = 10000
)
```

```{r sims_mc plot}
# plot sims
plot(sims_mc)
```

```{r sims_mc summary}
# summary
summary(sims_mc)
```
```{r sims_mc gelman-rubin}
# gelman-rubin statistic
gelman.diag(sims_mc)
gelman.plot(sims_mc)
```

```{r sims_mc autocorr}
# autocorrelation
autocorr.plot(sims_mc[[1]])
```

```{r 95 CI plot}
chains <- data.frame(sims_mc[[1]])

# 95% credible intervals
CI_time <- quantile(chains$mu_time, probs = c(0.025, 0.975))
CI_dist <- quantile(chains$mu_dist, probs = c(0.025, 0.975))
CI_cost <- quantile(chains$mu_cost, probs = c(0.025, 0.975))
CI_time
CI_dist
CI_cost

# 95% CI plots
# time
ggplot(chains, aes(x = mu_time)) +
  geom_density(fill = "darkgray", alpha = 0.2) +
  geom_vline(xintercept = CI_time, col = "red", lwd = 1) +
  ggtitle("Time") +
  xlab("Time (min)") +
  ylab("Density")

# distance
ggplot(chains, aes(x = mu_dist)) +
  geom_density(fill = "darkgray", alpha = 0.2) +
  geom_vline(xintercept = CI_dist, col = "red", lwd = 1) +
  ggtitle("Distance") +
  xlab("Distance (km)") +
  ylab("Density")

# cost
ggplot(chains, aes(x = mu_cost)) +
  geom_density(fill = "darkgray", alpha = 0.2) +
  geom_vline(xintercept = CI_cost, col = "red", lwd = 1) +
  ggtitle("Cost") +
  xlab("Cost (EUR)") +
  ylab("Density")
```

## Regression

```{r}
library('dplyr')
new_data <- read.csv("origin_weather_data.csv")
new_data <- replace(new_data, is.na(new_data), 0)
# length(unique(c(new_data$est_transport, new_data$est_transport)))
```

```{r}
cost_model <- "model{
  for (i in 1:length(Y)){
    Y[i] ~ dnorm(m[i], s^(-2))
    m[i] <- a + b*X1[i] + c*X2[i] + d*X3[i] + e*X4[i] + f*X5[i] + g*X6[i] + s
  }

  m0_dist <- 14.76
  s0_dist <- 17.71457
  m0_time <- 35.3169
  s0_time <- 29.52084

  a ~ dunif(0, 100) #intercept
  b ~ dnorm(m0_time, s0_time^(-2)) # time (min)
  c ~ dnorm(m0_dist, s0_dist^(-2)) # distance (km)
  d ~ dnorm(7, 2^(-2)) # temperature (degrees celcius)
  e ~ dnorm(6, 1^(-2)) # precipitation (mm)
  f ~ dnorm(0.75, 0.1^(-2)) # percent humidity
  g ~ dnorm(0.5, 0.1^(-2)) # wind speed (km/h)
  s ~ dunif(0, 100) # error
  }"

cost_model_jags <- jags.model(textConnection(cost_model),
  data = list(Y = new_data$cost,
    X1 = new_data$est_time,
    X2 = new_data$est_distance,
    X3 = new_data$temp_celcius,
    X4 = new_data$precipitation_mm,
    X5 = new_data$perc_humidity,
    X6 = new_data$wind_kmh),
  n.chains = 3)

sim_cost <- update(cost_model_jags, 10000)

n <- 10000
sims_cost <- coda.samples(cost_model_jags,
  variable.names = c("a", "b", "c", "d", "e", "f", "g", "s"),
  n.iter = n)
```

```{r}
summary(sims_cost)

plot(sims_cost)

gelman.diag(sims_cost)
gelman.plot(sims_cost)

autocorr.plot(sims_cost[[1]])

```

```{r}
time_model <- "model{
  for (i in 1:length(Y)){
    Y[i] ~ dnorm(m[i], s^(-2))
    m[i] <- a + b*X1[i] + c*X2[i] + d*X3[i] + e*X4[i] + f*X5[i] + g*X6[i] + s
  }

  m0_dist <- 14.76
  s0_dist <- 17.71457
  m0_cost <- 5.842254
  s0_cost <- 4.030727

  a ~ dunif(0, 100) #intercept
  b ~ dnorm(m0_cost, s0_cost^(-2)) # cost (EUR)
  c ~ dnorm(m0_dist, s0_dist^(-2)) # distance (km)
  d ~ dnorm(7, 2^(-2)) # temperature (degrees celcius)
  e ~ dnorm(6, 1^(-2)) # precipitation (mm)
  f ~ dnorm(0.75, 0.1^(-2)) # percent humidity
  g ~ dnorm(0.5, 0.1^(-2)) # wind speed (km/h)
  s ~ dunif(0, 100) # error
  }"

time_model_jags <- jags.model(textConnection(time_model),
  data = list(Y = new_data$cost,
    X1 = new_data$est_cost,
    X2 = new_data$est_distance,
    X3 = new_data$temp_celcius,
    X4 = new_data$precipitation_mm,
    X5 = new_data$perc_humidity,
    X6 = new_data$wind_kmh),
  n.chains = 3)

sim_time <- update(time_model_jags, 10000)

n <- 10000
sims_time <- coda.samples(time_model_jags,
  variable.names = c("a", "b", "c", "d", "e", "f", "g", "s"),
  n.iter = n)
```

```{r}
summary(sims_time)

plot(sims_time)

gelman.diag(sims_time)
gelman.plot(sims_time)

autocorr.plot(sims_time[[1]])
```

```{r}
dist_model <- "model{
  for (i in 1:length(Y)){
    Y[i] ~ dnorm(m[i], s^(-2))
    m[i] <- a + b*X1[i] + c*X2[i] + d*X3[i] + e*X4[i] + f*X5[i] + g*X6[i] + s
  }

  m0_time <- 35.3169
  s0_time <- 29.52084
  m0_cost <- 5.842254
  s0_cost <- 4.030727

  a ~ dunif(0, 100) #intercept
  b ~ dnorm(m0_cost, s0_cost^(-2)) # cost (EUR)
  c ~ dnorm(m0_time, s0_time^(-2)) # time (min)
  d ~ dnorm(7, 2^(-2)) # temperature (degrees celcius)
  e ~ dnorm(6, 1^(-2)) # precipitation (mm)
  f ~ dnorm(0.75, 0.1^(-2)) # percent humidity
  g ~ dnorm(0.5, 0.1^(-2)) # wind speed (km/h)
  s ~ dunif(0, 100) # error
  }"

dist_model_jags <- jags.model(textConnection(dist_model),
  data = list(Y = new_data$cost,
    X1 = new_data$est_cost,
    X2 = new_data$est_time,
    X3 = new_data$temp_celcius,
    X4 = new_data$precipitation_mm,
    X5 = new_data$perc_humidity,
    X6 = new_data$wind_kmh),
  n.chains = 3)

sim_dist <- update(dist_model_jags, 10000)

n <- 10000

sims_dist <- coda.samples(dist_model_jags,
  variable.names = c("a", "b", "c", "d", "e", "f", "g", "s"),
  n.iter = n)
```

```{r}
summary(sims_dist)

plot(sims_dist)

gelman.diag(sims_dist)
gelman.plot(sims_dist)

autocorr.plot(sims_dist[[1]])
```
